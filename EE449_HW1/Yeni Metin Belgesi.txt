import torchvision
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
print("berkay")
# training set
train_data = torchvision.datasets.FashionMNIST('./data', train = True, download = True,
transform = torchvision.transforms.ToTensor())
# test set
test_data = torchvision.datasets.FashionMNIST('./data', train = False,
transform = torchvision.transforms.ToTensor())
train_generator = torch.utils.data.DataLoader(train_data, batch_size = 96, shuffle = True)
test_generator = torch.utils.data.DataLoader(test_data, batch_size = 96, shuffle = False)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("ipek")
epoch_size = 10
# example mlp classifier
class FullyConnected(torch.nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(FullyConnected, self).__init__()
        self.input_size = input_size
        self.fc1 = torch.nn.Linear(input_size, hidden_size)
        self.fc2 = torch.nn.Linear(hidden_size, num_classes)
        self.relu = torch.nn.ReLU()
    def forward(self, x):
        x = x.view(-1, self.input_size)
        hidden = self.fc1(x)
        relu = self.relu(hidden)
        output = self.fc2(relu)
        return output
model_mlp = FullyConnected(784,128,10).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model_mlp.parameters(), lr = 0.01, momentum = 0.0)

total_step = len(train_generator)
train_losses=[]
train_accu=[]
for epoch in range(epoch_size):
    model_mlp.train()


    model_mlp.train()
    running_loss=0
    total=0
    correct=0
    training_loss=0
    loss_values=[]
    for i, (images, labels) in enumerate(train_generator): 
        
        # Move tensors to the configured device
        images = images.reshape(-1, 28*28).to(device) X
        labels = labels.to(device) X
        
        # Forward pass
        outputs = model_mlp(images) X
        loss = criterion(outputs, labels.to(device)) X
        
        # Backward and optimize
        optimizer.zero_grad() X
        loss.backward() X 
        optimizer.step() X
        running_loss = loss.item() X
        loss_values.append(running_loss) X
        _, predicted = outputs.max(1)
        total += labels.size(0) X 
        correct += predicted.eq(labels).sum().item()
        if (i+1) % 10 == 0:
            
            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' 
                   .format(epoch+1, epoch_size, i+1, total_step, loss.item()))
            print(running_loss)
            train_losses.append(running_loss)
    
    
    accu=100.*correct/total
    train_accu.append(accu)

    print('Train Loss: %.3f | Accuracy: %.3f'%(training_loss,accu))
    print('Finished Training Trainset')
eval_losses=[]
eval_accu=[]
with torch.no_grad():
    model_mlp.eval()
    running_loss =0
    correct = 0
    total = 0
    for images, labels in test_generator:
        images = images.reshape(-1, 28*28).to(device)
        labels = labels.to(device)
        outputs = model_mlp(images)
        loss= criterion(outputs, labels.to(device))
        running_loss+=loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
    test_loss=running_loss/len(test_generator)
    accu=100.*correct/total

    eval_losses.append(test_loss)
    eval_accu.append(accu)
    print('Test Loss: %.3f | Accuracy: %.3f'%(test_loss,accu)) 
plt.figure(0)
plt.plot(train_accu,'-o')
plt.plot(eval_accu,'-o')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend(['Train','Valid'])
plt.title('Train vs Valid Accuracy')
plt.figure(1)
plt.plot(train_losses,'-o')
plt.plot(eval_losses,'-o')
plt.xlabel('epoch')
plt.ylabel('losses')
plt.legend(['Train','Valid'])
plt.title('Train vs Valid Losses')
print("train acc",train_accu)
print("eval acc",eval_accu)
print("train loss",train_losses)
print("eval loss",eval_losses)

plt.show()

# Save the model checkpoint
torch.save(model_mlp.state_dict(), 'model.ckpt')